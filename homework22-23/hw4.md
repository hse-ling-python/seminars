## Домашнее задание 2: Краулер

**Дедлайн:**

**221 -- 28 ноября; 222 -- 24 ноября 23:59; 223 -- TBA**
 

**Важное: работы принимаются строго в ipynb (не скрипт, не скрипт в ячейке ipynb)**

**Задание**

Вам нужно обкачать новости (или статьи) с сайтов газет или аналогичных онлайн-изданий и записать это в датафрейм, затем сохранить как csv. **Точная формулировка по необходимым данным будет от преподавателя в зависимости от информации на выбранном сайте**

**Данные**

Надо записаться в таблицу, чтобы ваши газеты не повторялись: [таблица](https://docs.google.com/spreadsheets/d/1R42INz6Q87GaAJjhIMlvnzf3j_VZ47c3bW-z0TgvEH4/edit?usp=sharing)

**Совет по выбору**

Легче всего, если:

- навигация по страницам/выпускам/датам - под/над есть что-то вроде 1, 2, 3, ... 45, 46 по которым переход на страницы (например, Медуза - это плохой вариант, там кнопка "показать еще", The Village хороший вариант - там внизу навигация по страницам)
- в адресной строке будет навигация вроде <адрес>p=2 или  <адрес>page=2 или что-то такое, например, как на фикбуке ("https://ficbook.net/fanfiction/no_fandom/originals?p=2"). Это можно посмотреть, перейдя на вторую страницу интересующей ленты новостей.
- региональная газета - там попроще сайт и будет легче доставать информацию (у топовых газет много всего на скриптах и там сложно или невозможно доставать информацию)


**Минимальные требования по объему: 1000 постов/новостей/статей**, если не будет решено, что сайт сложный и можно меньше

**На 6 баллов**:

1. Выполнены требования по кол-ву постов
3. Данные сохраняются в датафрейм, есть описание данных
4. Посчитайте статистику:
   - самые популярные теги (или другой параметр в вашей газете)


**На 8 баллов**

1. Код программы разделен на логичные функции (а не все подряд в цикле)
2. Есть обработка ошибок (try-except), куда-то сохраняется информация о том, какие посты не скачались и какая ошибка
4. Построено три графика разных типов (line, pie, bar plot, scatter, word cloud), графики описаны.

**На 9 баллов**

Возьмите любую семантическую модель из тех, что мы проходили в первом модуле. Используя sklearn.metrics.pairwise.cosine_similarity, посчитайте близость между векторами текстов и для каждого текста найдите 3 похожих. Сохраните пары в файл, в тетрадке приведите результаты для 3 текстов (можно вывести только начало текста). 

**На 10 баллов**
Возьмите еще одну модель и сравните их качества. Визуализируйте результаты с помощью графиков, проанализируйте графики в текстовом комментармии.

**Подсказка** используйте библиотеку tqdm для удобного отслеживания прогресса

**Чек-лист**

1. Тетрадка с кодом
2. **Получившаяся база данных (она должна открываться и быть заполненной!)**, проверьте на адекватность хотя бы по размеру файла - он должен быть как минимм несколько мегабайт.

Если у вас есть вопросы, задавайте в чате или пишите преподавателям

**Ссылки на GiHub Classroom:**

[211](https://classroom.github.com/a/OafH8Fa6)

212

[213](https://classroom.github.com/a/y3MvaaAF)
