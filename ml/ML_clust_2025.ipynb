{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "969ebbda-9397-4502-992c-941ef834fd2d",
      "metadata": {
        "id": "969ebbda-9397-4502-992c-941ef834fd2d"
      },
      "source": [
        "# Кластеризация и снижение размерности"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c575b1-4a05-444e-b912-51b14bdf2dee",
      "metadata": {
        "id": "17c575b1-4a05-444e-b912-51b14bdf2dee"
      },
      "outputs": [],
      "source": [
        "# !pip install scikit-learn pandas -q\n",
        "!pip install hdbscan umap-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "2_ue5e6xz9_O"
      },
      "id": "2_ue5e6xz9_O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74ab6672-8e8a-4ed5-a164-d382d11d16b9",
      "metadata": {
        "id": "74ab6672-8e8a-4ed5-a164-d382d11d16b9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import homogeneity_completeness_v_measure\n",
        "\n",
        "from sklearn.cluster import (\n",
        "    KMeans,\n",
        "    DBSCAN,\n",
        "    AgglomerativeClustering\n",
        ")\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.decomposition import (\n",
        "    TruncatedSVD,\n",
        "    PCA\n",
        ")\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "y\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from scipy.stats import entropy\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# nltk.download('stopwords')\n",
        "stops = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53050b38-224c-47c5-92e5-ccf3e53acc4c",
      "metadata": {
        "id": "53050b38-224c-47c5-92e5-ccf3e53acc4c"
      },
      "source": [
        "[Датасет эмоций](https://www.kaggle.com/datasets/nelgiriyewithana/emotions?resource=download)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o emotions.zip https://www.kaggle.com/api/v1/datasets/download/nelgiriyewithana/emotions\n",
        "!unzip emotions.zip"
      ],
      "metadata": {
        "id": "Ng67E6z80IcF"
      },
      "id": "Ng67E6z80IcF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8299e422-6b8c-4d44-a5e6-0422d501b2ee",
      "metadata": {
        "id": "8299e422-6b8c-4d44-a5e6-0422d501b2ee"
      },
      "source": [
        "## Кластеризация"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9cbd5ae-f90e-475e-a1ff-2a3844cdab7b",
      "metadata": {
        "id": "c9cbd5ae-f90e-475e-a1ff-2a3844cdab7b"
      },
      "source": [
        "### Теория"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b105236-0593-48e6-9dfb-eb50a63a0459",
      "metadata": {
        "id": "0b105236-0593-48e6-9dfb-eb50a63a0459"
      },
      "source": [
        "**Кластеризация** - одна из классических задач обучения без учителя, в рамках которой требуется разбить множество объектов на группы по \"похожести\" так, чтобы объекты внутри группы были как можно более похожи, а группы между собой при этом максимально отличались.\n",
        "\n",
        "Кластеризацяи часто используется как способо анализа данных: в задаче анализа социальных сетей, тематического моделирования или просто как первый этап подготовки датасета для новой задачи, когда есть большой объем данных, но нет четкого понимания структуры классов. Однако, кластеризация также может быть и частью алгоритма решения более сложной задачи, чаще всего в свере построения рекомендательных систем.\n",
        "\n",
        "Группы объектов в данном случае называются **кластерами**. Часто для кластера вводя такое понятие, как **центроид** - определеная точка центра кластера (часто точка внутри кластера, равноудаленная от всех объектов кластера)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb77d4ae-5f83-40fe-bc29-bed9dfc76a1b",
      "metadata": {
        "id": "fb77d4ae-5f83-40fe-bc29-bed9dfc76a1b"
      },
      "source": [
        "***\n",
        "Алгоритмы кластеризации можно условно разделить на несколько групп:\n",
        "1. Неиерархические четкие алгоритмы - выделяют n кластреров так, что все кластеры равнозначны (нет иерархии) и при этом любой объект может принадлежать только к одному кластеру.\n",
        "<div>\n",
        "<img src=\"https://github.com/hse-ling-python/seminars/blob/master/ml/static/partitioning-clustering.webp?raw=true\" width=\"400\"/>\n",
        "</div>\n",
        "2. Иерархические четкие алгоритмы - объекты все еще не могут принадлежать более, чем к одному кластеру, однако алгоритм выстраивает иерархию вложенности для кластеров\n",
        "<div>\n",
        "<img src=\"https://github.com/hse-ling-python/seminars/blob/master/ml/static/hclust-example.png?raw=true\" width=\"400\"/>\n",
        "</div>\n",
        "3. Неиерархические нечеткие алгоритмы - выделяют n равнозначных кластреров, но для кажлого объекта определяется не один конкретный кластер, а распределние вероятностей принадлежать к каждому кластеру\n",
        "<div>\n",
        "<img src=\"https://github.com/hse-ling-python/seminars/blob/master/ml/static/normalcomparison.png?raw=true\" width=\"400\"/>\n",
        "</div>\n",
        "4. Иерархические нечеткие алгоритмы - строим иерархию для распределений вероятностей. Очень редкая штука (может, так вообще не делают, но я верю, что так можно)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea7d4281-67d0-4beb-a78c-7390de00d604",
      "metadata": {
        "id": "ea7d4281-67d0-4beb-a78c-7390de00d604"
      },
      "source": [
        "***\n",
        "Еще несмотря на то, что кластеризация - это обучение без учителя, существуют метрики для оценки кластеризации. Их условно можно разделить на две группы:\n",
        "1. Метрики, которые в действительности требуют правильных лейблов. Их обычно используют, чтобы настроить алгоритм кластеризации, который потом будет работать автономно на похожих данных, или для проверки нового алгоритма.\n",
        "    - Однородность (homogenity) - уменьшается, если в кластере оказываются объекты разных классов (аналог precision для классификации)\n",
        "<div>\n",
        "<img src=\"https://github.com/hse-ling-python/seminars/blob/master/ml/static/homo.png?raw=true\" width=\"400\"/>\n",
        "</div>\n",
        "    - Полнота (completeness) - уменьшается, если объекты одного класса оказываются в разных кластерах (аналог recall для классификации)\n",
        "<div>\n",
        "<img src=\"https://github.com/hse-ling-python/seminars/blob/master/ml/static/compl.png?raw=true\" width=\"400\"/>\n",
        "</div>\n",
        "    - V-measure - связыввет предыдущие два (аналог F-score для классификации)\n",
        "$$v = (1 + \\beta) * \\frac{homogeneity * completeness}{\\beta * homogeneity + completeness}$$\n",
        "    - Rag Bag\n",
        "<div>\n",
        "<img src=\"https://github.com/hse-ling-python/seminars/blob/master/ml/static/ragbag.png?raw=true\" width=\"400\"/>\n",
        "</div>\n",
        "2. Метрики, которые используют только свойства кластеров и объектов внутри них:\n",
        "    - Энтропия - \"оценка беспорядочности\". Максимальная для равномерного распределения, так как в нем нет никакой определенности\n",
        "\n",
        "    $$H = -\\sum_{i = 0}^n (x_{i} \\cdot \\log x_{i})$$\n",
        "\n",
        "    - Разные способы оценить \"плотность\" кластера: среднее отклонение от центроида, средня попарная близость между элементами и тд\n",
        "    - Оценки для \"разнообразия\" кластеров: расстояние между центроидами, расстояние между ближайшими элементами на границе и тд"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a6a755b-3ba6-4ce1-86d6-8f9999dab1ec",
      "metadata": {
        "id": "8a6a755b-3ba6-4ce1-86d6-8f9999dab1ec"
      },
      "outputs": [],
      "source": [
        "print(entropy(np.array([0.0, 1.0]) ))\n",
        "print(entropy(np.array([0.5, 0.5]) ))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1504377-a234-4674-a8c1-8efdd0bfc3b3",
      "metadata": {
        "id": "b1504377-a234-4674-a8c1-8efdd0bfc3b3"
      },
      "source": [
        "### Практика"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791cac61-c391-41ae-9056-5bf56253cabc",
      "metadata": {
        "id": "791cac61-c391-41ae-9056-5bf56253cabc"
      },
      "source": [
        "**Какие есть алгоритмы в питоне?**\n",
        "\n",
        "Вообще, множество. Большая часть из них живет в библиотеке sklearn, но есть отедльные, например, HDBSCAN.\n",
        "\n",
        "Мы сегодня посмотрим на K-means, DBSCAN, и AgglomerativeClustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11452ab-35d1-45eb-add1-2d18b457880d",
      "metadata": {
        "id": "d11452ab-35d1-45eb-add1-2d18b457880d"
      },
      "outputs": [],
      "source": [
        "labels = {\n",
        "    'sadness': 0,\n",
        "    'joy': 1,\n",
        "    'love': 2,\n",
        "    'anger': 3,\n",
        "    'fear': 4,\n",
        "    'surprise': 5\n",
        "}\n",
        "labels_list = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "df = pd.read_csv('text.csv').drop(columns=['Unnamed: 0']).sample(10000)\n",
        "df = df.reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a89c5fe5-bfc8-4081-ac30-8f023924d78c",
      "metadata": {
        "id": "a89c5fe5-bfc8-4081-ac30-8f023924d78c"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=stops).fit(df.text)\n",
        "vecs = tfidf.transform(df.text)\n",
        "vecs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d717d11-1e81-4ecd-b5a7-bcd11a4db2a9",
      "metadata": {
        "id": "6d717d11-1e81-4ecd-b5a7-bcd11a4db2a9"
      },
      "source": [
        "#### K-Means\n",
        "Алгоритм:\n",
        "1. Выбираем кол-во кластеров k (гиперпараметр, нужно назначить руками)\n",
        "2. Выбираем k случайных объектов. Они будут центроидами будущих кластеров\n",
        "3. Перебираем все объекты в данных, относим их к ближайшим центроидам\n",
        "4. Для каждого получившегося кластера считаем среднее значение. Эти значения будут новыми кластерами\n",
        "5. Повторяем шаги 3-4, пока центроиды не перестанут двигаться"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97b07175-11a2-40d8-9612-bb2378940bb0",
      "metadata": {
        "id": "97b07175-11a2-40d8-9612-bb2378940bb0"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=len(labels), random_state=0, n_init=\"auto\")\n",
        "kmeans.fit(vecs)\n",
        "\n",
        "preds = kmeans.labels_\n",
        "# homogeneity, completeness, v_measure\n",
        "homogeneity_completeness_v_measure(df.label, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b7e9f6-097e-42a3-9487-3ca1ec1899f5",
      "metadata": {
        "id": "f6b7e9f6-097e-42a3-9487-3ca1ec1899f5"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=100, random_state=0, n_init=\"auto\")\n",
        "kmeans.fit(vecs)\n",
        "\n",
        "preds = kmeans.labels_\n",
        "# homogeneity, completeness, v_measure\n",
        "homogeneity_completeness_v_measure(df.label, preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc1c525-1de3-4619-8f1a-70c27ff78115",
      "metadata": {
        "id": "2dc1c525-1de3-4619-8f1a-70c27ff78115"
      },
      "source": [
        "#### DBSCAN\n",
        "Интуиция: если между точками можно проложить \"плотную\" дорожку, то они относятся к одному кластеру\n",
        "\n",
        "Алгоритм (очень неформально):\n",
        "1. Берем случайную точку. Если рядом меньше нужного кол-ва точек, то считаем, что эта точка может быть шумом и идем к следующей. В итоге нам нужна точка, у которой достаточное кол-во соседей\n",
        "2. Для такой точки создаем группу, в которой она пока будет одна\n",
        "3. Обходим всех соседей точки. Если точка есть в списке возможного шума, то считаем, что это край группы. Если рядом есть другие \"популярные\" точки, то считаем их частью группы и перебираем и их соседей тоже\n",
        "4. Алгоритм заканчивается, когда мы тем или иным способом посмотрели на все точки\n",
        "5. В списке шума остался шум, остальное разделилось на кластеры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bd5b4ac-42a9-4342-a0eb-0b19d49bdb65",
      "metadata": {
        "id": "7bd5b4ac-42a9-4342-a0eb-0b19d49bdb65"
      },
      "outputs": [],
      "source": [
        "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
        "dbscan.fit(vecs)\n",
        "\n",
        "preds_db = dbscan.labels_\n",
        "# homogeneity, completeness, v_measure\n",
        "homogeneity_completeness_v_measure(df.label, preds_db)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee994c4b-4701-4b1e-a53c-978a6ec6f38b",
      "metadata": {
        "id": "ee994c4b-4701-4b1e-a53c-978a6ec6f38b"
      },
      "source": [
        "#### AgglomerativeClustering\n",
        "Интуиция: объединяем кластеры, пока можем это делать\n",
        "\n",
        "Алгоритм:\n",
        "1. Каждый объект представляет собой кластер\n",
        "2. Считаем попарную близость для всех кластеров\n",
        "3. Объединяем два с наибольшим значением\n",
        "4. Повторяем 2-3, пока не достигнут критерий остановки (чаще всего кол-во кластеров)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63cf7e9e-77c0-4472-aca2-c774bd132d48",
      "metadata": {
        "id": "63cf7e9e-77c0-4472-aca2-c774bd132d48"
      },
      "outputs": [],
      "source": [
        "agl = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
        "agl = agl.fit(vecs.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6e0c575-db4c-4cdb-a895-9dd0d13fc1bf",
      "metadata": {
        "id": "e6e0c575-db4c-4cdb-a895-9dd0d13fc1bf"
      },
      "outputs": [],
      "source": [
        "# отсюда: https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html\n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack(\n",
        "        [model.children_, model.distances_, counts]\n",
        "    ).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379a1603-37ad-4948-a36b-2a980bee8872",
      "metadata": {
        "id": "379a1603-37ad-4948-a36b-2a980bee8872"
      },
      "outputs": [],
      "source": [
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "plot_dendrogram(agl, truncate_mode=\"level\", p=3)\n",
        "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f1d8be4-cd75-4b68-9268-0acb2aa074db",
      "metadata": {
        "id": "0f1d8be4-cd75-4b68-9268-0acb2aa074db"
      },
      "source": [
        "## Снижение размерности"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2cb9393-d956-422a-a40b-686da9e77a08",
      "metadata": {
        "id": "b2cb9393-d956-422a-a40b-686da9e77a08"
      },
      "source": [
        "Здесь обсудим в общих чертах. Идея **снижения размерности** заключается в том, что иногда мы хотим перейти в пространство меньшей размерности (например, от 300 значений в эмбеддинге от word2vec к 2-м для визуализации) так, чтобы потерять как можно меньше информации об отношениях между объектами.\n",
        "\n",
        "Такие алгоритмы могут быть построены двумя способами (возможно, есть еще, но эти основные):\n",
        "1. Отбор признаков - выбираем нужное кол-во признаков из имеющихся (часто используется в МЛ)\n",
        "2. Проекция признаков - преобразуем исходное кол-во характеристик в новое так, что новые характеристики представляют собой некоторую агрегацию старых\n",
        "\n",
        "Популярные алгоритмы:\n",
        "- SVD (Singular Value Decomposition)\n",
        "- UMAP (Uniform Manifold Approximation and Projection)\n",
        "- TSNE (T-distributed Stochastic Neighbor Embedding)\n",
        "- PCA (Principal Component Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cur = df[df.label.isin([0, 2])]\n",
        "cur_vecs = vecs[cur.index, :]"
      ],
      "metadata": {
        "id": "to254NyC1xxl"
      },
      "id": "to254NyC1xxl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9b9df76-ebf9-478c-8cb8-069382a788d3",
      "metadata": {
        "id": "a9b9df76-ebf9-478c-8cb8-069382a788d3"
      },
      "outputs": [],
      "source": [
        "reducer = umap.UMAP(n_neighbors=20, min_dist=0.5, n_components=2)\n",
        "\n",
        "embedding = reducer.fit_transform(cur_vecs)\n",
        "print(embedding.shape)\n",
        "\n",
        "plt.scatter(\n",
        "    embedding[:, 0],\n",
        "    embedding[:, 1],\n",
        "    c=cur.label)\n",
        "plt.gca().set_aspect('equal', 'datalim')\n",
        "plt.title('UMAP projection of data', fontsize=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68a00d04-7a66-4224-9d0f-969f185d2e71",
      "metadata": {
        "id": "68a00d04-7a66-4224-9d0f-969f185d2e71"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}