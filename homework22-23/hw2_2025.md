## Домашнее задание 2

### **Тема: Морфология (щепотка таблиц и визуализаций)**

#### **Мягкий дедлайн**: 24.10.2025 (пт) 23:59
#### **Жесткий дедлайн** (половина оценки): 31.10.2025 (пт) 23:59
Сдавайте задание заблаговременно! 00:00 - это уже не 23:59 :)
<br>

#### **Данные**
Каждый выбирает себе свою книгу в формате TXT, например на [http://lib.ru/](http://lib.ru/). Берите не рассказ, а большое произведение $-$ так будет интереснее. 

(Если ваша любимая книга доступна только в FB2 / EPUB и т.д., то можно конвертировать ее в TXT при помощи любого [онлайн-сервиса](https://convertio.co/ru/fb2-txt/) или самому найти ее в TXT где-нибудь еще)

<br>

#### **Задание**
Обработать текст книги с помощью морфологических анализаторов, сделать поверхностный анализ и визуализацию.

1. Выбрать и сохранить книгу в `.txt`.
2. (2 балла) Обработать книгу через NLTK + Pymorphy:
    - токенизировать текст с помощью NLTK и убрать стоп-слова,
    - разобрать слова с помощью Pymorphy,
    - сохранить результат в JSON Lines (`.jsonl`), где каждая строчка $-$ это разбор слова в виде словаря<br>`{"word": "коня", "lemma": "конь", "pos": "NOUN"}`,
    - создать датафрейм частотности лемм в вашем тексте, отсортировать его по убыванию и сохранить в формате `.csv`. В нем должны быть столбцы *lemma*, *pos* и *frequency* с начальной формой слова, ее частью речи и частотностью этой леммы в тексте соответственно. Например, начало вашего CSV файла может выглядеть так:
		```
		lemma,pos,frequency
		конь,NOUN,25
		```
3. (2 балла) Обработать книгу с помощью Spacy:
    - токенизировать и разобрать слова с помощью Spacy,
    - сохранить результат в JSON Lines (аналогично п. 2)
    - создать датафрейм частотности лемм в вашем тексте (аналогично п. 2)

4. (3 балла) Анализ и визуализация
    - При помощи pandas посчитать, какую долю слов составляет каждая часть речи. (Например, для глагола $-$ это количество глаголов, деленное на общее число слов в тексте.) Сравнить результаты для Spacy и Pymorphy и прокомментировать их.<br>
	- Визуализировать полученные вами подсчеты при помощи подходящего графика.
    - Также при помощи pandas найти топ-20 глаголов и наречий по частотности. Сравнить результаты для Spacy и Pymorphy и прокомментировать их.<br>
	- Визуализировать частотность этих двух топов-20 при помощи подходящего графика.
    - Построить облако слов для вашего токенизированного и лемматизированного текста.<br>
      Можно посмотреть [документацию](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud.WordCloud), чтобы исключить одно- или двухбуквенные слова.

5. (1 балл) Посмотреть документацию для NLTK n-грамм (например, `nltk.bigrams`) и составить топ-25 биграмм и триграмм для вашего текста в лемматизированном виде (только леммы, без знаков препинания). Почему получаются именно такие?
6. (2 балла) Взять абзац из изначального текста (3-8 предложений) и заменить в нём:
	- число всех существительных (единственное на множественное, множественное на единственное),
	- время всех глаголов (на любое другое), 
	- согласовать по числу глаголы и существительные. 
	Например, вместо изначального *Слон подарил мартышке цветы* должно получиться *Слоны подарят мартышкам цветок*.
<br>

**Что должно быть загружено в classroom?**
- Тетрадка с кодом, графиками и ответами на вопросы
- Текст книги
- Два файла JSON Lines (`.jsonl`) c результатом работы NLTK+Pymorphy и Spacy
- Два файла CSV c частотностью лемм по результатам работы NLTK+Pymorphy и Spacy

<br>

**Некоторые полезные соображения**
- Если есть проблемы с загрузкой файла в питон, проверьте, что кодировка UTF-8
- Работа с JSON Lines была в тетрадке с первого семинара
- Если у вас не импортируется NLTK, попробуйте установить заново, запустить заново тетрадку. Если не работает, то спросите в чате!

<br>

**Ссылки на GiHub Classroom: TBA**
