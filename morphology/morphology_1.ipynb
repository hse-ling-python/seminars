{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Морфология\n",
    "#### План семинара:\n",
    "\n",
    "1. Mystem консольный\n",
    "2. Mystem через python\n",
    "3. Pymorphy\n",
    "4. NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужные пакеты для этого семинара:\n",
    "\n",
    "``pip install pymystem3``\n",
    "\n",
    "``pip install pymorphy2``\n",
    "\n",
    "Если вы хотите побыстрее и у вас Linux или Mac\n",
    "\n",
    "``pip install pymorphy2[fast]``\n",
    "\n",
    "``pip install nltk``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mystem - это свободно распространяемый морфологический анализатор для русского языка с закрытым исходным кодом.\n",
    "\n",
    "My-stem значит my stemmer, стемминг -- это разбиение формы на основу и флексию. На самом деле Mystem может гораздо больше: устанавливать словарную форму слова, определять часть речи и грамматическую форму слова. В последних версиях Mystem умеет и выбирать из нескольких возможных грамматических разборов один, наиболее верный.\n",
    "\n",
    "У Mystem нет графического оконного интерфейса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mystem консольный\n",
    "\n",
    "**Preface:**\n",
    "\n",
    "Мы уже устанавливали jupyter-notebook c помощью командной строки. Для этого нужно было вызывать программу pip.exe и передавали туда команду install и параметр jupyter. Программа для морфологического анализа Mystem тоже запускается через командную строку и тоже требует передачи определённых *аргументов* (или ещё их называют *параметрами*). Тогда эти аргументы пишутся после пути к программе через пробел. Если сам аргумент тоже содержит пробел, его нужно обернуть в кавычки: `C:\\some_programm.exe argument1 \"argument 2\"`\n",
    "\n",
    "Среди аргументов есть такие, которые принято называть *опциями* или *ключами*, они начинаются с символа дефиса: `C:\\some_prog.exe -a -b`. Их можно \"склеивать\": `C:\\some_prog.exe -ab`.\n",
    "\n",
    "**Как скачать**\n",
    "\n",
    "Скачать Mystem можно [отсюда](https://tech.yandex.ru/mystem/), а [тут](https://tech.yandex.ru/mystem/doc/index-docpage/) лежит его документация (там описаны различные возможности вызова программы).\n",
    "\n",
    "Примеры, которые приведены в документации, рассчитаны на пользователя Unix-подобной операционной системы. В начале примеров вызова стоит знак доллара, $. Доллар -- это аналог приглашения командной строки, просто не в Windows, а в Unix-подобных системах. Если вы берете примеры вызова со страницы документации за основу, игнорируйте знак доллара.\n",
    "\n",
    "В документации написано: `$ mystem input`. На практике для пользователей Windows это будет значить что-то вроде `C:\\mystem.exe input.txt`. \n",
    "\n",
    "В документации написано \"стандартный ввод\" и \"стандартный вывод\", это значит то, что вводится в командной строке или выводится в тот же терминал. Если не используются стандартный ввод и вывод, то используются файлы (выводной файл Mystem способен создать сам).\n",
    "\n",
    "В 3-й версии Mystem кодировка по умолчанию -- utf-8. В первых версиях -- cp1251. Кодировка по умолчанию в командной строке Windows -- cp866. Из-за этого Mystem может не понимать слова, которые попадают к нему из стандартного ввода.\n",
    "\n",
    "**Как запустить**\n",
    "\n",
    "Особое внимание нужно уделить опции `-d`, она заставляет анализатор выбирать только один разбор из возможных. При этом выбор происходит только между разными частями речи. Если у одной части речи возможны разные разборы (например, разные падежи одного и того же существительного), то эти разборы не отбрасываются. Иначе говоря, Mystem снимает только частеречную омонимию. Омонимию форм он не снимает.\n",
    "\n",
    "**Пример запуска из командной строки**\n",
    "\n",
    "Возьмите файл experiment.txt и запустите его так, чтобы для каждого слова была снята омонимия и напечатана грамматическая информация.\n",
    "\n",
    "Посмотрим, как Mystem справится с глокой куздрой и бокрёнком."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mystem через python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для запуска сторонних программ, имеющих интерфейс командной строки, в питоне есть system, предоставляемая модулем os: `os.system(\"C:\\mystem.exe input.txt output.txt\")`. Но не забудьте импортировать модуль os.\n",
    "\n",
    "Этот код берёт из директории *input_texts* все лежащие в ней файлы, отдаёт на разметку майстему и кладёт результат в соседнюю директорию *output_texts*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "inp = \"./input_files\"\n",
    "outp = \"./output_files\"\n",
    "mystem_path = os.path.join('/Users/johola/Downloads', 'mystem')\n",
    "files = os.listdir(inp)\n",
    "for fname in files:\n",
    "    # os.path.abspath - находит абсолютный путь\n",
    "    # os.path.join - объединяет части пути до файла\n",
    "    input_filename = os.path.join(os.path.abspath(inp), fname)\n",
    "    output_filename = os.path.join(os.path.abspath(outp), fname)\n",
    "    os.system(f\"{mystem_path} {input_filename} {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А что там на самом деле пишется? Посмотрим на последних переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/johola/Downloads\\\\mystem C:\\\\Users\\\\johola\\\\seminars\\\\morphology\\\\input_files\\\\nabokov_5.txt C:\\\\Users\\\\johola\\\\seminars\\\\morphology\\\\output_files\\\\nabokov_5.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{mystem_path} {input_filename} {output_filename}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, этот способ записи называется f-string и позволяет собирать строчки по шаблону"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это тоже можно попробовать. Для примера у нас в папке input_texts лежат нарезанные тексты Набокова Дар"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно запускать mystem и с помощью специального модуля, **pymystem3**. Это проще и удобнее, потому что с тем, что выдаёт mystem, можно сразу работать как с питоновскими структурами данных. Но медленнее. Иногда гораздо-гораздо медленнее, чем разметить один файл mystem'ом сразу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing mystem to C:\\Users\\johola/.local/bin\\mystem.exe from http://download.cdn.yandex.net/mystem/mystem-3.1-win-64bit.zip\n"
     ]
    }
   ],
   "source": [
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У этого класса Mystem() есть два метода:\n",
    "\n",
    "* lemmatize, возвращающий список лемм,\n",
    "* и analyze, возвращающий полные разборы в виде словаря.\n",
    "\n",
    "Возьмем небольшой текст и опробуем на нем эти два метода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Но не становится ли событие тем значительнее и исключительнее,\" +\\\n",
    "\"чем большее число случайностей приводит к нему?\" +\\\n",
    "\" Лишь случайность может предстать перед нами как послание.\" +\\\n",
    "\" Все, что происходит по необходимости, что ожидаемо, что повторяется всякий день, то немо.\" +\\\n",
    "\" Лишь случайность о чем-то говорит нам. Мы стремимся прочесть ее, \" +\\\n",
    "\"как читают цыганки по узорам, начертанным кофейной гущей на дне чашки.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['тем', ' ', 'значительный', ' ', 'и', ' ', 'исключительный', ',', 'чем', ' ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = m.lemmatize(text)\n",
    "lemmas[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно собрать лемматизированный текст обратно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "но не становиться ли событие тем значительный и исключительный,чем больший число случайность приводить к он? лишь случайность мочь представать перед мы как послание. все, что происходить по необходимость, что ожидать, что повторяться всякий день, то немо. лишь случайность о что-то говорить мы. мы стремиться прочитывать она, как читать цыганка по узор, начертать кофейный гуща на дно чашка.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(''.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'analysis': [{'gr': 'CONJ=', 'lex': 'но', 'wt': 0.9998906299}], 'text': 'Но'},\n",
      " {'text': ' '},\n",
      " {'analysis': [{'gr': 'PART=', 'lex': 'не', 'wt': 1}], 'text': 'не'},\n",
      " {'text': ' '},\n",
      " {'analysis': [{'gr': 'V,нп=непрош,ед,изъяв,3-л,несов',\n",
      "                'lex': 'становиться',\n",
      "                'wt': 1}],\n",
      "  'text': 'становится'},\n",
      " {'text': ' '},\n",
      " {'analysis': [{'gr': 'PART=', 'lex': 'ли', 'wt': 0.7719288688}], 'text': 'ли'},\n",
      " {'text': ' '},\n",
      " {'analysis': [{'gr': 'S,сред,неод=(вин,ед|им,ед)', 'lex': 'событие', 'wt': 1}],\n",
      "  'text': 'событие'},\n",
      " {'text': ' '},\n",
      " {'analysis': [{'gr': 'CONJ=', 'lex': 'тем', 'wt': 0.0857739759}],\n",
      "  'text': 'тем'},\n",
      " {'text': ' '},\n",
      " {'analysis': [{'gr': 'A=срав', 'lex': 'значительный', 'wt': 0.2062520859}],\n",
      "  'text': 'значительнее'},\n",
      " {'text': ' '},\n",
      " {'analysis': [{'gr': 'CONJ=', 'lex': 'и', 'wt': 0.9999770357}], 'text': 'и'},\n",
      " {'text': ' '},\n",
      " {'analysis': [{'gr': 'A=срав', 'lex': 'исключительный', 'wt': 1}],\n",
      "  'text': 'исключительнее'},\n",
      " {'text': ','},\n",
      " {'analysis': [{'gr': 'CONJ=', 'lex': 'чем', 'wt': 0.8023791472}],\n",
      "  'text': 'чем'},\n",
      " {'text': ' '}]\n"
     ]
    }
   ],
   "source": [
    "ana = m.analyze(text)\n",
    "pprint(ana[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбор для каждого слова является элементом массива:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': [{'lex': 'но', 'wt': 0.9998906299, 'gr': 'CONJ='}], 'text': 'Но'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'не', 'wt': 1, 'gr': 'PART='}], 'text': 'не'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'становиться', 'wt': 1, 'gr': 'V,нп=непрош,ед,изъяв,3-л,несов'}], 'text': 'становится'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'ли', 'wt': 0.7719288688, 'gr': 'PART='}], 'text': 'ли'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'событие', 'wt': 1, 'gr': 'S,сред,неод=(вин,ед|им,ед)'}], 'text': 'событие'}\n",
      "{'text': ' '}\n"
     ]
    }
   ],
   "source": [
    "for word in ana[:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разборе в поле text можно найти исходное слова, а в поле analysis (которого может и не быть) - грамматические характеристики и леммы.\n",
    "\n",
    "В грамматическом разборе знаком = отделяются изменяемые характеристики от неизменяемых. Знаком | отделяются омонимичные разборы.\n",
    "\n",
    "Достанем все части речи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Но CONJ\n",
      "не PART\n",
      "становится V\n",
      "ли PART\n",
      "событие S\n",
      "тем CONJ\n",
      "значительнее A\n",
      "и CONJ\n",
      "исключительнее A\n",
      "чем CONJ\n",
      "большее A\n",
      "число S\n",
      "случайностей S\n",
      "приводит V\n",
      "к PR\n",
      "нему SPRO\n",
      "Лишь PART\n",
      "случайность S\n",
      "может V\n",
      "предстать V\n",
      "перед PR\n",
      "нами SPRO\n",
      "как CONJ\n",
      "послание S\n",
      "Все SPRO\n",
      "что CONJ\n",
      "происходит V\n",
      "по PR\n",
      "необходимости S\n",
      "что CONJ\n",
      "ожидаемо V\n",
      "что CONJ\n",
      "повторяется V\n",
      "всякий APRO\n",
      "день S\n",
      "то CONJ\n",
      "немо ADV\n",
      "Лишь PART\n",
      "случайность S\n",
      "о PR\n",
      "чем-то SPRO\n",
      "говорит V\n",
      "нам SPRO\n",
      "Мы SPRO\n",
      "стремимся V\n",
      "прочесть V\n",
      "ее SPRO\n",
      "как ADVPRO\n",
      "читают V\n",
      "цыганки S\n",
      "по PR\n",
      "узорам S\n",
      "начертанным V\n",
      "кофейной A\n",
      "гущей S\n",
      "на PR\n",
      "дне S\n",
      "чашки S\n"
     ]
    }
   ],
   "source": [
    "for word in ana:\n",
    "    if 'analysis' in word:\n",
    "        gr = word['analysis'][0]['gr']\n",
    "        pos = gr.split('=')[0].split(',')[0]\n",
    "        print(word['text'], pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зачем нам тогда что-то ещё?\n",
    "\n",
    "**Достоинства Mystem'a:**\n",
    "\n",
    "- хорошее качество разбора\n",
    "- по умолчанию разрешается частеречная омонимия (внутри части речи остается)\n",
    "- при разборе учитывается контекст\n",
    "- совместим с разметкой НКРЯ\n",
    "\n",
    "**Недостатки Mystem'a:**\n",
    "\n",
    "- медленный\n",
    "- analyze возвращает неудобный json\n",
    "\n",
    "Разберёмся подробнее с pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может делать то же, что и pymystem3, и даже больше: изменять слова в нужную форму (спрягать и склонять). При этом pymorphy2 справляется и с незнакомыми словами.\n",
    "\n",
    "[Документация](https://pymorphy2.readthedocs.io/en/latest/)\n",
    "\n",
    "Для работы точно так же надо создать экземпляр класса MorphAnalyzer. Рекомендуется создать один экземпляр и дальше с ним и работать, поскольку он занимает достаточно много памяти, и если создать несколько экземпляров анализаторов, то они будут тормозить программу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбор слова делается при помощи метода parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='стекло', score=0.690476, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 0),)),\n",
       " Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='стекло', score=0.285714, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 3),)),\n",
       " Parse(word='стекло', tag=OpencorporaTag('VERB,perf,intr neut,sing,past,indc'), normal_form='стечь', score=0.023809, methods_stack=((DictionaryAnalyzer(), 'стекло', 1015, 3),))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana = morph.parse('стекло')\n",
    "ana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, анализатор вернул все возможные разборы этого слова, отранжировав их по вероятности.\n",
    "\n",
    "У каждого разбора есть атрибуты: исходное слово, тэг, лемма, вероятность разбора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово: стекло\n",
      "Тэг: NOUN,inan,neut sing,nomn\n",
      "Лемма: стекло\n",
      "Вероятность: 0.690476\n"
     ]
    }
   ],
   "source": [
    "first = ana[0]  # первый разбор\n",
    "print('Слово:', first.word)\n",
    "print('Тэг:', first.tag)\n",
    "print('Лемма:', first.normal_form)\n",
    "print('Вероятность:', first.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого разбора можно получить лемму и всю информацию о ней (т.е. еще один разбор, только уже для леммы):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='стекло', score=0.690476, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 0),))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first.normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разбор слова:  Parse(word='стекло', tag=OpencorporaTag('VERB,perf,intr neut,sing,past,indc'), normal_form='стечь', score=0.023809, methods_stack=((DictionaryAnalyzer(), 'стекло', 1015, 3),))\n",
      "\n",
      "Разбор леммы:  Parse(word='стечь', tag=OpencorporaTag('INFN,perf,intr'), normal_form='стечь', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стечь', 1015, 0),))\n"
     ]
    }
   ],
   "source": [
    "last = ana[-1] # последний разбор\n",
    "print('Разбор слова: ', last)\n",
    "print()\n",
    "print('Разбор леммы: ', last.normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если распечатать тег разбора, то может показаться, что это строка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN,inan,neut sing,nomn\n"
     ]
    }
   ],
   "source": [
    "first = ana[0]  # первый разбор\n",
    "print(first.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но на самом деле это объект класса OpencorporaTag, так что некоторые вещи, которые можно делать со строками, с тэгами делать нельзя. А некоторые все-таки можно.\n",
    "\n",
    "Например, можно проверить, есть ли какая-то граммема в теге:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'NOUN' in first.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'VERB' in first.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'NOUN', 'inan'} in first.tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из каждого тега можно достать более дробную информацию. Если граммема есть в разборе, то вернется ее значение, если ее нет, то вернется None."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p.tag.POS           # Part of Speech, часть речи\n",
    "p.tag.animacy       # одушевленность\n",
    "p.tag.aspect        # вид: совершенный или несовершенный\n",
    "p.tag.case          # падеж\n",
    "p.tag.gender        # род (мужской, женский, средний)\n",
    "p.tag.involvement   # включенность говорящего в действие\n",
    "p.tag.mood          # наклонение (повелительное, изъявительное)\n",
    "p.tag.number        # число (единственное, множественное)\n",
    "p.tag.person        # лицо (1, 2, 3)\n",
    "p.tag.tense         # время (настоящее, прошедшее, будущее)\n",
    "p.tag.transitivity  # переходность (переходный, непереходный)\n",
    "p.tag.voice         # залог (действительный, страдательный)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB,perf,intr neut,sing,past,indc\n",
      "Время:  past\n",
      "Падеж:  None\n"
     ]
    }
   ],
   "source": [
    "print(last.tag)\n",
    "print('Время: ', last.tag.tense)\n",
    "print('Падеж: ', last.tag.case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список граммем, которые используются в модуле, находится здесь - https://pymorphy2.readthedocs.io/en/latest/user/grammemes.html.\n",
    "\n",
    "Если искать какую-то граммему, которой нет в этом списке, возникнет ошибка.\n",
    "\n",
    "Можно получить строку с кириллическими обозначениями граммем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'СУЩ,неод,ср ед,им'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first.tag.cyr_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Словоизменение**\n",
    "\n",
    "Если у нас есть разбор слова, то мы можем это слово поставить в другую форму с помощью функции inflect. Эта функция получает на вход множество граммем и пытается применить их к нашему разбору."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='программирую', tag=OpencorporaTag('VERB,impf,tran sing,1per,pres,indc'), normal_form='программировать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'программирую', 171, 1),))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('программирую')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='программируем', tag=OpencorporaTag('VERB,impf,tran plur,1per,pres,indc'), normal_form='программировать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'программируем', 171, 2),))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = morph.parse('программирую')[0]\n",
    "prog.inflect({'plur'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='программировали', tag=OpencorporaTag('VERB,impf,tran plur,past,indc'), normal_form='программировать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'программировали', 171, 10),))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog.inflect({'plur', 'past'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='программировал', tag=OpencorporaTag('VERB,impf,tran masc,sing,past,indc'), normal_form='программировать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'программировал', 171, 7),))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog.inflect({'past'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'программировала'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog.inflect({'past', 'femn'})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Формы слова**\n",
    "\n",
    "С помощью атрибута lexeme можно получить массив всех форм слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='программировать', tag=OpencorporaTag('INFN,impf,tran'), normal_form='программировать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'программировать', 171, 0),)),\n",
       " Parse(word='программирую', tag=OpencorporaTag('VERB,impf,tran sing,1per,pres,indc'), normal_form='программировать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'программирую', 171, 1),)),\n",
       " Parse(word='программируем', tag=OpencorporaTag('VERB,impf,tran plur,1per,pres,indc'), normal_form='программировать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'программируем', 171, 2),)),\n",
       " Parse(word='программируешь', tag=OpencorporaTag('VERB,impf,tran sing,2per,pres,indc'), normal_form='программировать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'программируешь', 171, 3),)),\n",
       " Parse(word='программируете', tag=OpencorporaTag('VERB,impf,tran plur,2per,pres,indc'), normal_form='программировать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'программируете', 171, 4),))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog.lexeme[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Согласование слов с числительными**\n",
    "\n",
    "Из документации:\n",
    "\n",
    "    Слово нужно ставить в разные формы в зависимости от числительного, к которому оно относится. Например: “1 бутявка”, “2 бутявки”, “5 бутявок” Для этих целей используйте метод Parse.make_agree_with_number():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "butyavka = morph.parse('бутявка')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бутявка'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "butyavka.make_agree_with_number(1).word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бутявки'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "butyavka.make_agree_with_number(2).word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бутявок'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "butyavka.make_agree_with_number(5).word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Саммари**\n",
    "\n",
    "**Достоинства Pymorphy:**\n",
    "\n",
    "- умеет составлять разборы, находить лемму, склонять и спрягать\n",
    "- генерирует гипотезы для незнакомых слов\n",
    "- написан полностью на питоне и быстрее, чем Mystem (и есть ускоренная версия с вставками на c++)\n",
    "- может работать с украинским языком (но словари нужно отдельно устанавливать)\n",
    "\n",
    "**Недостатки Pymorphy:**\n",
    "\n",
    "- качество хуже, чем у Mystem\n",
    "- работает только на уровне отдельных слов (и естественно, не учитывает контекст)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Небольшой хак**\n",
    "\n",
    "Pymorphy и так работает очень быстро, но можно еще быстрее, если мы будем сохранять разборы для очень популярных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input_files/nabokov.txt', encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "text = [word.lower().strip(punctuation) for word in text.split()]\n",
    "text = [word for word in text if word != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.19 s ± 140 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "lemmas = []\n",
    "\n",
    "for word in text:\n",
    "    lemmas.append(morph.parse(word)[0].normal_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.41 s ± 5.74 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "lemmas = []\n",
    "known_words = {}\n",
    "\n",
    "for word in text:\n",
    "    if word in known_words:\n",
    "        lemmas.append(known_words[word])\n",
    "    else:\n",
    "        result = morph.parse(word)[0].normal_form\n",
    "        lemmas.append(result)\n",
    "        known_words[word] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы просто запоминаем леммы и поэтому не парсим слово каждый раз, а берем из быстрого хранилища готовый результат. Это может серьезно загружать память (при больших объемах), но значительно сократит время работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typing_extensions==4.7.1\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: typing_extensions\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "Successfully installed typing_extensions-4.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install typing_extensions==4.7.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ru-core-news-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.6.0/ru_core_news_sm-3.6.0-py3-none-any.whl (15.3 MB)\n",
      "     ---------------------------------------- 15.3/15.3 MB 8.5 MB/s eta 0:00:00\n",
      "Collecting pymorphy3>=1.0.0\n",
      "  Downloading pymorphy3-1.2.1-py3-none-any.whl (55 kB)\n",
      "     -------------------------------------- 55.4/55.4 kB 728.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from ru-core-news-sm==3.6.0) (3.6.1)\n",
      "Collecting docopt-ng>=0.6\n",
      "  Downloading docopt_ng-0.9.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.6.0) (0.7.2)\n",
      "Collecting pymorphy3-dicts-ru\n",
      "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
      "     ---------------------------------------- 8.4/8.4 MB 12.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (4.64.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2.28.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2.4.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (65.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (22.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (1.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: pydantic-core==2.10.0 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\johola\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\johola\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2.1.1)\n",
      "Installing collected packages: pymorphy3-dicts-ru, docopt-ng, pymorphy3, ru-core-news-sm\n",
      "Successfully installed docopt-ng-0.9.0 pymorphy3-1.2.1 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.6.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple рассматривает возможность покупки стартапа из Соединённого Королевства за $1 млрд Беспилотные автомобили перекладывают страховую ответственность на производителя В Сан-Франциско рассматривается возможность запрета роботов-курьеров, которые перемещаются по тротуару Лондон — это большой город в Соединённом Королевстве\n",
      "Apple PROPN nsubj\n",
      "рассматривает VERB ROOT\n",
      "возможность NOUN obj\n",
      "покупки NOUN nmod\n",
      "стартапа NOUN nmod\n",
      "из ADP case\n",
      "Соединённого ADJ amod\n",
      "Королевства PROPN nmod\n",
      "за ADP case\n",
      "$ NOUN nmod\n",
      "1 NUM nummod\n",
      "млрд NOUN nmod\n",
      "Беспилотные ADJ amod\n",
      "автомобили NOUN nsubj\n",
      "перекладывают VERB conj\n",
      "страховую ADJ amod\n",
      "ответственность NOUN obj\n",
      "на ADP case\n",
      "производителя NOUN nmod\n",
      "В ADP case\n",
      "Сан PROPN nmod\n",
      "- PROPN nmod\n",
      "Франциско PROPN nmod\n",
      "рассматривается VERB conj\n",
      "возможность NOUN nsubj:pass\n",
      "запрета NOUN nmod\n",
      "роботов NOUN nmod\n",
      "- NOUN nmod\n",
      "курьеров NOUN nmod\n",
      ", PUNCT punct\n",
      "которые PRON nsubj\n",
      "перемещаются VERB acl:relcl\n",
      "по ADP case\n",
      "тротуару NOUN obl\n",
      "Лондон PROPN appos\n",
      "— PUNCT punct\n",
      "это PART expl\n",
      "большой ADJ amod\n",
      "город NOUN appos\n",
      "в ADP case\n",
      "Соединённом ADJ amod\n",
      "Королевстве PROPN nmod\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.ru.examples import sentences \n",
    "\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "doc = nlp(\" \".join(sentences[:4]))\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это уже не просто морфологический анализатор, а целая NLP библиотека!\n",
    "\n",
    "Что мы тут можем делать? Можем тоекнизировать какой-нибудь текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Такой зеленый, серый, то есть\n",
    "весь заштрихованный дождем,\n",
    "и липовое, столь густое,\n",
    "что я перенести - уйдем!\n",
    "Уйдем и этот сад оставим\n",
    "и дождь, кипящий на тропах\n",
    "между тяжелыми цветами,\n",
    "целующими липкий прах.\n",
    "Уйдем, уйдем, пока не поздно,\n",
    "скорее, под плащом, домой,\n",
    "пока еще ты не опознан,\n",
    "безумный мой, безумный мой!\n",
    "\n",
    "Держусь, молчу. Но с годом каждым,\n",
    "под гомон птиц и шум ветвей,\n",
    "разлука та обидней кажется,\n",
    "обида кажется глупей.\n",
    "И все страшней, что опрометчиво\n",
    "проговорюсь и перебью\n",
    "теченье тихой, трудной речи,\n",
    "давно проникшей в жизнь мою.\n",
    "\n",
    "Над краснощекими рабами\n",
    "лазурь как лаковая вся,\n",
    "с накачанными облаками,\n",
    "едва заметными толчками\n",
    "передвигающимися.\n",
    "Ужель нельзя там притулиться\n",
    "и нет там темного угла,\n",
    "где темнота могла бы слиться\n",
    "с иероглифами крыла?\n",
    "Так бабочка не шевелится\n",
    "пластом на плесени ствола.\n",
    "\n",
    "Какой закат! И завтра снова,\n",
    "и долго-долго быть жаре,\n",
    "что безошибочно основано\n",
    "на тишине и мошкаре.\n",
    "В луче вечернем повисая,\n",
    "она толчется без конца,\n",
    "как бы игрушка золотая\n",
    "в руках немого продавца.\n",
    "\n",
    "Как я люблю тебя. Есть в этом\n",
    "вечернем воздухе порой\n",
    "лазейки для души, просветы\n",
    "в тончайшей ткани мировой.\n",
    "Лучи проходят меж стволами.\n",
    "Как я люблю тебя! Лучи\n",
    "проходят меж стволами, пламенем\n",
    "ложатся на стволы. Молчи.\n",
    "Замри под веткою расцветшей,\n",
    "вдохни, какое разлилось -\n",
    "зажмурься, уменьшись и в вечное\n",
    "пройди украдкою насквозь.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johola\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Такой', 'зеленый', ',', 'серый', ',', 'то', 'есть', 'весь', 'заштрихованный', 'дождем', ',', 'и', 'липовое', ',', 'столь', 'густое', ',', 'что', 'я', 'перенести', '-', 'уйдем', '!', 'Уйдем', 'и', 'этот', 'сад', 'оставим', 'и', 'дождь', ',', 'кипящий', 'на', 'тропах', 'между', 'тяжелыми', 'цветами', ',', 'целующими', 'липкий', 'прах', '.', 'Уйдем', ',', 'уйдем', ',', 'пока', 'не', 'поздно', ',', 'скорее', ',', 'под', 'плащом', ',', 'домой', ',', 'пока', 'еще', 'ты', 'не', 'опознан', ',', 'безумный', 'мой', ',', 'безумный', 'мой', '!', 'Держусь', ',', 'молчу', '.', 'Но', 'с', 'годом', 'каждым', ',', 'под', 'гомон', 'птиц', 'и', 'шум', 'ветвей', ',', 'разлука', 'та', 'обидней', 'кажется', ',', 'обида', 'кажется', 'глупей', '.', 'И', 'все', 'страшней', ',', 'что', 'опрометчиво', 'проговорюсь', 'и', 'перебью', 'теченье', 'тихой', ',', 'трудной', 'речи', ',', 'давно', 'проникшей', 'в', 'жизнь', 'мою', '.', 'Над', 'краснощекими', 'рабами', 'лазурь', 'как', 'лаковая', 'вся', ',', 'с', 'накачанными', 'облаками', ',', 'едва', 'заметными', 'толчками', 'передвигающимися', '.', 'Ужель', 'нельзя', 'там', 'притулиться', 'и', 'нет', 'там', 'темного', 'угла', ',', 'где', 'темнота', 'могла', 'бы', 'слиться', 'с', 'иероглифами', 'крыла', '?', 'Так', 'бабочка', 'не', 'шевелится', 'пластом', 'на', 'плесени', 'ствола', '.', 'Какой', 'закат', '!', 'И', 'завтра', 'снова', ',', 'и', 'долго-долго', 'быть', 'жаре', ',', 'что', 'безошибочно', 'основано', 'на', 'тишине', 'и', 'мошкаре', '.', 'В', 'луче', 'вечернем', 'повисая', ',', 'она', 'толчется', 'без', 'конца', ',', 'как', 'бы', 'игрушка', 'золотая', 'в', 'руках', 'немого', 'продавца', '.', 'Как', 'я', 'люблю', 'тебя', '.', 'Есть', 'в', 'этом', 'вечернем', 'воздухе', 'порой', 'лазейки', 'для', 'души', ',', 'просветы', 'в', 'тончайшей', 'ткани', 'мировой', '.', 'Лучи', 'проходят', 'меж', 'стволами', '.', 'Как', 'я', 'люблю', 'тебя', '!', 'Лучи', 'проходят', 'меж', 'стволами', ',', 'пламенем', 'ложатся', 'на', 'стволы', '.', 'Молчи', '.', 'Замри', 'под', 'веткою', 'расцветшей', ',', 'вдохни', ',', 'какое', 'разлилось', '-', 'зажмурься', ',', 'уменьшись', 'и', 'в', 'вечное', 'пройди', 'украдкою', 'насквозь', '.']\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 14 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "%time print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем разделить текст на предложения (сплиттинг):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nТакой зеленый, серый, то есть\\nвесь заштрихованный дождем,\\nи липовое, столь густое,\\nчто я перенести - уйдем!',\n",
       " 'Уйдем и этот сад оставим\\nи дождь, кипящий на тропах\\nмежду тяжелыми цветами,\\nцелующими липкий прах.',\n",
       " 'Уйдем, уйдем, пока не поздно,\\nскорее, под плащом, домой,\\nпока еще ты не опознан,\\nбезумный мой, безумный мой!',\n",
       " 'Держусь, молчу.',\n",
       " 'Но с годом каждым,\\nпод гомон птиц и шум ветвей,\\nразлука та обидней кажется,\\nобида кажется глупей.',\n",
       " 'И все страшней, что опрометчиво\\nпроговорюсь и перебью\\nтеченье тихой, трудной речи,\\nдавно проникшей в жизнь мою.',\n",
       " 'Над краснощекими рабами\\nлазурь как лаковая вся,\\nс накачанными облаками,\\nедва заметными толчками\\nпередвигающимися.',\n",
       " 'Ужель нельзя там притулиться\\nи нет там темного угла,\\nгде темнота могла бы слиться\\nс иероглифами крыла?',\n",
       " 'Так бабочка не шевелится\\nпластом на плесени ствола.',\n",
       " 'Какой закат!',\n",
       " 'И завтра снова,\\nи долго-долго быть жаре,\\nчто безошибочно основано\\nна тишине и мошкаре.',\n",
       " 'В луче вечернем повисая,\\nона толчется без конца,\\nкак бы игрушка золотая\\nв руках немого продавца.',\n",
       " 'Как я люблю тебя.',\n",
       " 'Есть в этом\\nвечернем воздухе порой\\nлазейки для души, просветы\\nв тончайшей ткани мировой.',\n",
       " 'Лучи проходят меж стволами.',\n",
       " 'Как я люблю тебя!',\n",
       " 'Лучи\\nпроходят меж стволами, пламенем\\nложатся на стволы.',\n",
       " 'Молчи.',\n",
       " 'Замри под веткою расцветшей,\\nвдохни, какое разлилось -\\nзажмурься, уменьшись и в вечное\\nпройди украдкою насквозь.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK может удалять стоп слова. Стоп-слова - это высокочастотные союзы, предлоги и другие служебные части речи, которые не дают нам никакой информации о конкретном тексте. В NLTK есть готовые списки стоп-слов (да-да, и для русского тоже есть)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# загружаем нужный список стоп-слов\n",
    "sw = stopwords.words('russian')\n",
    "\n",
    "# смотрим, что внутри\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['зеленый', 'серый', 'весь', 'заштрихованный', 'дождем', 'липовое', 'столь', 'густое', 'перенести', 'уйдем', 'уйдем', 'сад', 'оставим', 'дождь', 'кипящий', 'тропах', 'тяжелыми', 'цветами', 'целующими', 'липкий', 'прах', 'уйдем', 'уйдем', 'пока', 'поздно', 'скорее', 'плащом', 'домой', 'пока', 'опознан', 'безумный', 'безумный', 'держусь', 'молчу', 'годом', 'каждым', 'гомон', 'птиц', 'шум', 'ветвей', 'разлука', 'та', 'обидней', 'кажется', 'обида', 'кажется', 'глупей', 'страшней', 'опрометчиво', 'проговорюсь', 'перебью', 'теченье', 'тихой', 'трудной', 'речи', 'давно', 'проникшей', 'жизнь', 'мою', 'краснощекими', 'рабами', 'лазурь', 'лаковая', 'вся', 'накачанными', 'облаками', 'едва', 'заметными', 'толчками', 'передвигающимися', 'ужель', 'притулиться', 'темного', 'угла', 'темнота', 'могла', 'слиться', 'иероглифами', 'крыла', 'бабочка', 'шевелится', 'пластом', 'плесени', 'ствола', 'закат', 'завтра', 'снова', 'жаре', 'безошибочно', 'основано', 'тишине', 'мошкаре', 'луче', 'вечернем', 'повисая', 'толчется', 'конца', 'игрушка', 'золотая', 'руках', 'немого', 'продавца', 'люблю', 'вечернем', 'воздухе', 'порой', 'лазейки', 'души', 'просветы', 'тончайшей', 'ткани', 'мировой', 'лучи', 'проходят', 'меж', 'стволами', 'люблю', 'лучи', 'проходят', 'меж', 'стволами', 'пламенем', 'ложатся', 'стволы', 'молчи', 'замри', 'веткою', 'расцветшей', 'вдохни', 'какое', 'разлилось', 'зажмурься', 'уменьшись', 'вечное', 'пройди', 'украдкою', 'насквозь']\n"
     ]
    }
   ],
   "source": [
    "# токенизируем текст, приводим к нижнему регистру и оставляем только последовательности из букв,\n",
    "# т.е. все токены, где были знаки препинания и числа, исчезнут\n",
    "words = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "\n",
    "# какие слова исчезли?\n",
    "filtered = [w for w in words if w not in sw]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И наконец-то стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# умеет работать не только с английским текстом\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "snowball = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бабочка: бабочк\n",
      "без: без\n",
      "безошибочно: безошибочн\n",
      "безумный: безумн\n",
      "бы: бы\n",
      "быть: быт\n",
      "в: в\n",
      "вдохни: вдохн\n",
      "весь: ве\n",
      "ветвей: ветв\n",
      "веткою: ветк\n",
      "вечернем: вечерн\n",
      "вечное: вечн\n",
      "воздухе: воздух\n",
      "все: все\n",
      "вся: вся\n",
      "где: где\n",
      "глупей: глуп\n",
      "годом: год\n",
      "гомон: гомон\n",
      "густое: густ\n",
      "давно: давн\n",
      "для: для\n",
      "дождем: дожд\n",
      "дождь: дожд\n",
      "долго-долго: долго-долг\n",
      "домой: дом\n",
      "души: душ\n",
      "едва: едв\n",
      "есть: ест\n"
     ]
    }
   ],
   "source": [
    "ruswords = set(word_tokenize(text))\n",
    "\n",
    "for w in sorted(ruswords)[20:50]:\n",
    "    print(\"%s: %s\" % (w, snowball.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По-моему, качество -- просто дно.\n",
    "\n",
    "А в лемматизации тут нет русского, но в целом good to know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\johola\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\johola\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "wnl.lemmatize('running', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "Текст - первая глава произведения \"Дар\" Набокова. Файл nabokov.txt у нас в репозитории.\n",
    "\n",
    "1. Первое задание - это небольшой эксперимент. Возьмите пять любых абзацев из текста и распарсите их двумя способами. 1) просто через pymystem (.analyze) и 2) через pymystem, предварительно почистив текст от пунктуации (тоже .analyze). Замерить, что из этого быстрее с помощью line magic ``%time some_python_expression_here``\n",
    "2. Токенизируйте весь текст с помощью nltk. \n",
    "3. Почистите его от знаков препинания (тут пригодится список из первого задания), стоп-слов (с помощью nltk) и слов не на кириллице. Сделайте регистр lower у всх слов.\n",
    "4. Лемматизируйте с помощью pymorphy (.normal_form)\n",
    "5. Cоставьте частотный список слов. Выведите 20 самых частотных слов вообще.\n",
    "6. Найдите 20 самых частотных существительных.\n",
    "7. В тексте (списку слов), полчившемся после пункта 3 (токенизированному и почищенному), поищите биграммы. Для этого нужно будет посмотреть nltk документацию про nltk.bigrams(). Выведите 10 самых частотных биграммов.\n",
    " \n",
    "Напоминание:\n",
    "\n",
    "**N-граммы** — это сочетания из N элементов (слов, символов), идущих друг за другом. Одиночные элементы называются униграммами, сочетания из двух элементов — биграммами, из трёх — триграммами, а дальше все пишется цифрами: 4-граммы, 5-граммы и т.д."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
