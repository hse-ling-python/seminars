## Домашнее задание 3

### Тема: синтаксис, семантика и инструменты

**Мягкий дедлайн**: 10.11.2025 (пн), 23:59

**Жесткий дедлайн** (половина оценки): 17.11.2025 (пн), 23:59

Сдавайте задание заблаговременно! 00:00 - это уже не 23:59 :)

#### **Данные**

Возьмите 5 статей из Википедии следующим образом: 
1. Выберите одну статью с понравившейся вам темой;
2. Перейдите по двум ссылкам внутри статьи $-$ это будет еще две ваши статьи;
3. От каждой из этих двух новых статей перейдите еще раз по какой-нибудь ссылке внутри.

Вам нужен текст этих статей, его можно получить через специальную библиотеку для работы с Википедией:

```python
!pip install wikipedia

import wikipedia

# Устанавливаем язык
wikipedia.set_lang("ru")

# Получаем статью по названию
page = wikipedia.page("Лингвистика")

# Достаем текст и название
title = page.title
text = page.content
```
Или скопировать руками...

#### **Задание**
Сравнить статьи с точки зрения семантической схожести по ключевым словам и посмотреть, какой способ извлекать ключевые слова дает более хороший результат.

1. Сохранить выбранные статьи в формате `.json`, файл должен иметь следующую структуру: `{title: text}` (словарь, где ключи - это названия статей, а значения - их тексты);

2. (1 балл) Дальше для векторизации через word2vec необходимо будет привести слова в выделенных именных группах/ключевых фразах к виду `лемма_ЧАСТЬ РЕЧИ` (например, `кот_NOUN`). Делайте это через любую удобную вам библиотеку, но помните о том, что большая часть word2vec моделей ждет теги как в Universal Dependencies. За правильный формат во всех заданиях суммарно будет 1 балл.

3. (2 балла) Извлечь из статей ключевые слова двумя способами:
    - С помощью KeyBert
    - С помощью YAKE

    Векторизовать их с помощью word2vec (если это ключевая фраза, то считайте средний вектор слов) и сохранить вектора в словарь вида `{key_word: vector}`;

4. (2 балла) Выделить все именные группы из статей:
    - Сделать синтаксический анализ с помощью spacy;
    - Для каждого существительного выделить его группу. Давайте считать, что минимально в это понятие входят прилагательные и существительные в родительном падеже, зависящие от текущего существительного. Но если сделаете точнее (на еще уровень вглубь по дереву: например, "кошка с белым котенком" научиться находить), то будет до +1 балла (да-да, можно получить 11 за домашку);
    - Векторизовать группы с помощью word2vec (считайте средний вектор слов) и сохранить вектора в словарь вида `{key_word: vector}`;

5. Сравнить выделенные ключевые слова и группы между:
    - (1 балл) Просто по текстовому совпадению: насколько совпадают ключи в словарях с результатами. Сравните для одной статьи разные способы извлечения между собой. Сделайте выводы о схожести трех результатов: KeyBert, YAKE и именные группы;
    - (2 балла) Через семантическую близость. Посчитайте для статей между собой и сделайте выводы о схожести тематик статей: есть ли какая-то закономерность между полученными значениями и смысловой схожестью статей? 
    <br>

    Как это сделать на примере одной пары статей:
    
        1. Для первой именной группы из одной статьи посчитать ее косинусную близость со всеми именными группами из другой статьи;
        2. Взять максимум из полученных значений, сохранить в список;
        3. Повторить предыдущие пункты для каждой именной группы;
        4. Поулчившийся в результате список значений усреднить и считать получившееся число оценкой близости между статьями.

    **Как вы считаете, в чем идея такого сравнения? Что оно позволяет нам увидеть?**

6. (2 балла, задание на 9-10) Выделить ключевые слова через Tf-Idf:

    > При работе с tf-idf можно считать, что чем выше значение в ячейке матрицы, тем важнее слово для понимания смысла текста. Таким образом, если взять топ-5 слов с наибольшими значениями, можно получить список ключевых слов. 

    - Векторизовать статьи с помощью Tf-Idf;
	- Для каждого текста (каждой строки матрицы) найти топ-5 максимальных значений (вам может пригодиться метод `argsort` из библиотеки `numpy`);
    - Найти слова в словаре Tf-Idf, которые соответствуют этим значениям;
    - Сравнить полученные ключевые слова с тем, что получилось в YAKE, KeyBert и именных группах.

Если хочется более интересных резльтатов от Tf-Idf, то можно почитать про параметр `ngram_range` [в документации](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).


**Что должно быть загружено в classroom?**
- Тетрадка с кодом и ответами на вопросы;
- Файл с текстами статей;


**Ссылки на GiHub Classroom:**

TBA