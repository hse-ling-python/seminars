## Домашнее задание 1

### **Тема: Морфология и визуализация**

#### **Дедлайн**: 

**Важное**: работы принимаются строго в IPYNB (не скрипт, не скрипт в ячейке IPYNB), после дедлайна работы не принимаются.
#### **Данные**
Каждый выбирает себе свою книгу в формате TXT, которая есть в (условно) открытом доступе на [http://lib.ru/](http://lib.ru/): это и следующее задания будут на материале выбранного текста. Давайте брать не рассказы, а большое произведение (книга) $-$ так будет интереснее. 
Если ваша любимая книга доступна только в FB2 / EPUB и т.д., то можно конвертировать ее в TXT при помощи любого онлайн-сервиса ([например](https://convertio.co/ru/fb2-txt/)) или самому найти ее в TXT где-нибудь еще.

#### **Задание**
Обработать текст книги с помощью морфологического анализатора, сделать поверхностный анализ и визуализацию.

**Пункты**:
1. Выбрать и сохранить книгу в `.txt`. Проследить, что кодировка UTF-8.
2. Обработать книгу с помощью Spacy:
    - токенизировать и разобрать слова с помощью Spacy
    - сохранить результат в JSON Lines (`.jsonl`), где каждая строчка $-$ это разбор слова в виде словаря<br>`{"lemma": "конь", "word": "коня", "pos": "NOUN"}`
    - создать датафрейм частотности лемм в вашем тексте, отсортировать его по убыванию и сохранить в формате `.csv`. В нем должны быть столбцы *lemma*, *pos* и *frequency*, а строчки с начальной формой слова, ее частью речи и ее частотностью в тексте. Например, начало вашего CSV файла может выглядеть так:
		```
		lemma,pos,frequency
		конь,NOUN,25
		```
    - (не на оценку, опционально)<br>
      замерить время работы на примере небольшого кусочка текста, например, главы
3. Обработать книгу через Pymorphy
    - токенизировать текст с помощью NLTK и убрать стоп-слова
    - разобрать слова с помощью Pymorphy
    - сохранить результат в JSON Lines (`.jsonl`), где каждая строчка $-$ это разбор слова в виде словаря<br>`{"lemma": "конь", "word": "коня", "pos": "NOUN"}`
    - создать датафрейм частотности лемм в вашем тексте, отсортировать его по убыванию и сохранить в формате `.csv`. В нем должны быть столбцы *lemma*, *pos* и *frequency*, а строчки с начальной формой слова, ее частью речи и частотностью этой леммы в тексте. Например, начало вашего CSV файла может выглядеть так:
		```
		lemma,pos,frequency
		конь,NOUN,25
		```
	- (не на оценку, опционально)<br>
      замерить время работы как со Spacy
4. Анализ и визуализация:
    - При помощи pandas посчитать, какую долю слов составляет каждая часть речи. (Например, для глагола $-$ это количество глаголов, деленное на общее число слов в тексте.) Сравнить результаты для Spacy и Pymorphy и прокомментировать их.<br>
      Визуализировать полученные вами подсчеты при помощи подходящего графика.
    - Также при помощи pandas найти топ-20 глаголов и наречий по частотности. Сравнить результаты для Spacy и Pymorphy и прокомментировать их.<br>
      Визуализировать частотность этих двух топов-20 при помощи подходящего графика.
    - Построить облако слов для вашего токенизированного и лемматизированного текста.<br>
      Можно посмотреть [документацию](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud.WordCloud), чтобы исключить одно- или двухбуквенные слова.
5. Посмотреть документацию для NLTK n-грамм (например, `nltk.bigrams`) и составить топ-25 биграмм и триграмм для вашего текста в лемматизированном виде (только леммы, без знаков препинания). Почему получаются именно такие? 
6. Взять абзац из изначального текста (3-8 предложений) и заменить в нём:
	- время всех глаголов (например, прошедшее на настоящее или будущее), 
	- число всех существительных (единственное на множественное, множественное на единственное), 
	- согласовать по числу глаголы и существительные. 
	Например, вместо изначального *Слон подарил мартышке цветы* должно получиться *Слоны подарят мартышкам цветок*.

**Что должно быть при сдаче?**
1. Тетрадка с кодом, графиками и ответами на вопросы
2. Текст книги
3. Два файла JSON Lines (`.jsonl`) c результатом работы Spacy и Pymorphy
4. Два CSV c частотностью лемм по результатам работы Spacy и Pymorphy

**Подсказка №1**. Используйте ``%%time`` для замеров времени. Для этого в начале ячейки, где находится то, что вы хотите замерить, напишите ``%%time``. Например:
```python
%%time

analysis = m.analyze('Вот так можно замерить время')
```

**Подсказка №2**. Если у вас не импортируется NLTK, попробуйте установить заново, запустить заново тетрадку. Если не работает, то спросите в чате.

**Подсказка №3**. Если у вас не импортируется Pymorphy, то проверьте вашу версию Python. Если вы работаете с Python версии 3.11+, то устанавливайте `pymorphy3`:
```python
!pip install pymorphy3 --q

from pymorphy3 import MorphAnalyzer
```
Это то же самое, но поддерживаются более новые версии Python. Или работайте в Google Colab: он на версии 3.10.

Если у вас есть вопросы, задавайте в чате или пишите преподавателям
#### Критерии оценки
Является обязательным оформление домашней работы в Jupyter Notebook с ответами на вопросы, комментариями и по PEP-8.
<table>
    <tr><th>Макс. балл</th><th>Критерий</th></tr>
    <tr><td>2</td><td>пункт 2 (с двумя файлами)</td></tr>    
    <tr><td>2</td><td>пункт 3 (с двумя файлами)</td></tr> 
    <tr><td>3</td><td>пункт 4 (с графиками и комментариями)</td></tr> 
    <tr><td>1</td><td>пункт 5 (с ответом на вопрос)</td></tr> 
    <tr><td>2</td><td>пункт 6</td></tr> 
</table>

**Ссылки на GiHub Classroom:**

<table>
    <tr><td>Группа 1</td><td>TBA</td></tr>
    <tr><td>Группа 2</td><td>TBA</td></tr>
    <tr><td>Группа 3</td><td>TBA</td></tr>       
</table>
